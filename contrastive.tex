\section{Contrastive learning}
%\chapter{Contrastive learning}

Как обучать encoder, не имея размеченных данных?
Можно придумать такие задачи, для который разметка может быть получена автоматически.
Например, восстановление повреждённого тем или иным способом изображения, восстановление вырезанного участка изображения,
раскраска изображения, переведённого в чёрно-белый, предсказание относительной позиции двух участков изображения,
предсказание угла, на который повёрнуто изображение, сравнительное обучение (contrastive learning). 
Подобные задачи выделяют в специальный раздел~--- self-supervised learning (самообучение).

Лучше всего (в смысле качества работы encoder-а после дообучения на конкретную задачу обучения с учителем)
себя показывает contrastive learning, поэтому поговорим о нём поподробнее.
Общая идея заключается в том, что мы хотим для похожих объектов выучить представления, 
которые будут близки в признаковом пространстве, а представления непохожих объектов должны быть далеко друг от друга.

Одним из первых примеров функции потерь, которая формализует это требование, является Contrastive loss:
\begin{equation*}
    \mathcal{L}_\text{contr}(x, x', \theta) =
    \begin{cases}
        \Vert f_\theta(x) - f_\theta(x') \Vert^2, \quad x' \text{~--- позитивный пример}\\
        \max \bigl(0, M - \Vert f_\theta(x) - f_\theta(x') \Vert^2\bigr), \quad x' \text{~--- негативный пример}\\
    \end{cases}
\end{equation*}

Позитивным примером называется объект, который мы считаем похожим на $x$ 
(например, в случае изображений можно взять горизонтально отражённый $x$, так как в большинстве случаев это не меняет семантику).
Отрицательный (негативный) пример, соответственно, есть объект, непохожий на $x$. 
В качестве отрицательных примеров обычно берут случайное изображение из датасета.

Сейчас часто используется Noise Contrastive Estimation.
\begin{equation*}
    \mathcal{L}\bigl(\hat{z}_t, z_t, \{z_l\}\bigr) = \log \left( \frac{\exp(\hat{z}_t^T z_t)}{ \exp(\hat{z}_t^T z_t) + \sum_l \exp(\hat{z}_t^T z_l)} \right)
\end{equation*}
Здесь $\hat{z}_t$~--- описание исходного объекта, $z_t$~--- позитивный пример, $\{z_l\}$~--- множество отрицательных примеров.
На это можно смотреть как на классификацию из $N + 1$ классов, где $N$~--- число негативных примеров.

Как оказалось, чтобы этот подход заработал, нужно большое количество негативных примеров~--- 
в статье SimCLR, в которой был предложен этот метод, использовались батчи из 2048 изображений 
(аугментированная версия $i$-го изображения рассматривалась как позитивный пример для него же, а все остальные~--- как негативные).